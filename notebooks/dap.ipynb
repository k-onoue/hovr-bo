{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# 1. Define Distance-Aware Priors\n",
    "class DistanceAwarePrior:\n",
    "    def __init__(self, sigma0):\n",
    "        self.sigma0 = sigma0  # Base standard deviation\n",
    "\n",
    "    def compute_distance(self, x_train, x_val):\n",
    "        \"\"\"Compute the nearest neighbor distance (identity map as feature extractor).\"\"\"\n",
    "        # Compute Euclidean distances\n",
    "        x_train_np = x_train.cpu().numpy()\n",
    "        x_val_np = x_val.cpu().numpy()\n",
    "        distances = np.min(cdist(x_val_np, x_train_np, 'euclidean'), axis=1)\n",
    "        return torch.tensor(distances, dtype=torch.float32)\n",
    "\n",
    "    def distance_aware_variance(self, distances, phi):\n",
    "        \"\"\"Compute the distance-aware variance.\"\"\"\n",
    "        return (self.sigma0 + torch.exp(phi) * distances).pow(2)\n",
    "\n",
    "# 2. Importance Sampling for Posterior Predictive\n",
    "class ImportanceSamplingPredictor:\n",
    "    def __init__(self, posterior_samples, prior_sigma0):\n",
    "        self.posterior_samples = posterior_samples  # Samples from posterior p(theta | x, y)\n",
    "        self.prior_sigma0 = prior_sigma0\n",
    "\n",
    "    def compute_importance_weights(self, distance_variances):\n",
    "        \"\"\"Compute importance weights to correct predictions.\"\"\"\n",
    "        # Gaussian weight ratios between distance-aware prior and posterior\n",
    "        prior_variances = self.prior_sigma0**2\n",
    "        weights = torch.exp(-0.5 * (1 / distance_variances - 1 / prior_variances))\n",
    "        return weights\n",
    "\n",
    "    def predictive_mean_and_variance(self, x_val, model, weights):\n",
    "        \"\"\"Estimate the predictive mean and variance using importance sampling.\"\"\"\n",
    "        preds = []\n",
    "        for theta in self.posterior_samples:  # Iterate through posterior samples\n",
    "            model.load_state_dict(theta)\n",
    "            with torch.no_grad():\n",
    "                preds.append(model(x_val).squeeze())  # Ensure output is 1D for each x_val\n",
    "        preds = torch.stack(preds, dim=0)  # Shape: [num_samples, num_inputs]\n",
    "        weighted_preds = preds * weights.unsqueeze(0)\n",
    "        mean = weighted_preds.mean(dim=0)  # Weighted mean across samples\n",
    "        variance = ((preds - mean.unsqueeze(0)) ** 2 * weights.unsqueeze(0)).mean(dim=0)\n",
    "        return mean, variance\n",
    "\n",
    "\n",
    "# 3. Calibration of Distance-Aware Priors\n",
    "class DAPCalibration:\n",
    "    def __init__(self, model, prior_sigma0, x_train, x_val):\n",
    "        self.model = model\n",
    "        self.prior = DistanceAwarePrior(prior_sigma0)\n",
    "        self.x_train = x_train\n",
    "        self.x_val = x_val\n",
    "        self.phi = torch.tensor(0.0, requires_grad=True)  # Initialize phi\n",
    "\n",
    "    def calibration_loss(self, distances, target_uncertainty=1.0):\n",
    "        \"\"\"Calibration loss to match target uncertainty.\"\"\"\n",
    "        distance_variances = self.prior.distance_aware_variance(distances, self.phi)\n",
    "        return torch.mean((distance_variances.sqrt() - target_uncertainty) ** 2)\n",
    "\n",
    "    def calibrate(self, target_uncertainty=1.0, lr=0.01, epochs=100):\n",
    "        \"\"\"Optimize phi using calibration loss.\"\"\"\n",
    "        optimizer = optim.Adam([self.phi], lr=lr)\n",
    "        distances = self.prior.compute_distance(self.x_train, self.x_val)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.calibration_loss(distances, target_uncertainty)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {loss.item():.4f}, Phi = {self.phi.item():.4f}\")\n",
    "\n",
    "        print(f\"Calibration complete. Optimal Phi = {self.phi.item():.4f}\")\n",
    "        return self.phi.detach()\n",
    "\n",
    "# 4. Example Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Toy Example Setup\n",
    "    class SimpleBNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc = nn.Sequential(nn.Linear(1, 10), nn.Tanh(), nn.Linear(10, 1))\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.fc(x)\n",
    "\n",
    "    # Generate Toy 1D Data\n",
    "    torch.manual_seed(42)\n",
    "    x_train = torch.cat([\n",
    "        torch.linspace(-3, -2, 5),  # Imbalanced regions\n",
    "        torch.linspace(0, 1, 10),\n",
    "        torch.linspace(2, 3, 5)\n",
    "    ]).unsqueeze(1)\n",
    "    y_train = torch.sin(x_train) + 0.1 * torch.randn_like(x_train)\n",
    "    x_val = torch.linspace(-6, 6, 100).unsqueeze(1)  # Out-of-distribution points\n",
    "\n",
    "    # Initialize BNN\n",
    "    bnn = SimpleBNN()\n",
    "\n",
    "    # Generate fake posterior samples (e.g., variational inference or MCMC)\n",
    "    posterior_samples = [bnn.state_dict() for _ in range(10)]\n",
    "\n",
    "    # Step 1: Calibrate Distance-Aware Priors\n",
    "    dap = DAPCalibration(bnn, prior_sigma0=1.0, x_train=x_train, x_val=x_val)\n",
    "    phi_opt = dap.calibrate(target_uncertainty=1.0)\n",
    "\n",
    "    # Step 2: Apply Importance Sampling for Predictions\n",
    "    predictor = ImportanceSamplingPredictor(posterior_samples, prior_sigma0=1.0)\n",
    "    distances = dap.prior.compute_distance(x_train, x_val)\n",
    "    distance_variances = dap.prior.distance_aware_variance(distances, phi_opt)\n",
    "    weights = predictor.compute_importance_weights(distance_variances)\n",
    "    predictive_mean, predictive_variance = predictor.predictive_mean_and_variance(x_val, bnn, weights)\n",
    "\n",
    "\n",
    "\n",
    "    # Convert predictions and intervals to NumPy\n",
    "    x_val_np = x_val.squeeze().numpy()\n",
    "    x_train_np = x_train.squeeze().numpy()\n",
    "    y_train_np = y_train.squeeze().numpy()\n",
    "    pred_mean_np = predictive_mean.detach().numpy()\n",
    "    pred_var_np = predictive_variance.detach().numpy()\n",
    "\n",
    "    # Compute confidence intervals\n",
    "    lower = pred_mean_np - 1.96 * np.sqrt(pred_var_np)\n",
    "    upper = pred_mean_np + 1.96 * np.sqrt(pred_var_np)\n",
    "\n",
    "    # Plot using Plotly\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Training data points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_train_np,\n",
    "            y=y_train_np,\n",
    "            mode=\"markers\",\n",
    "            name=\"Training Data\",\n",
    "            marker=dict(color=\"blue\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Predictive mean\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_val_np,\n",
    "            y=pred_mean_np,\n",
    "            mode=\"lines\",\n",
    "            name=\"Predictive Mean\",\n",
    "            line=dict(color=\"red\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Confidence interval as a filled band\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.concatenate([x_val_np, x_val_np[::-1]]),\n",
    "            y=np.concatenate([upper, lower[::-1]]),\n",
    "            fill=\"toself\",\n",
    "            fillcolor=\"rgba(255, 0, 0, 0.2)\",\n",
    "            line=dict(width=0),\n",
    "            name=\"95% Confidence Interval\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Layout settings\n",
    "    fig.update_layout(\n",
    "        title=\"Distance-Aware Prior Calibration\",\n",
    "        xaxis_title=\"Input x\",\n",
    "        yaxis_title=\"Output y\",\n",
    "        legend_title=\"Legend\",\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_hovr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
